{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.43.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement rquests (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for rquests\u001b[0m\n",
      "Requirement already satisfied: toposort in /usr/local/lib/python3.7/dist-packages (1.5)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2020.2.20)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.18.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.6.1->pandas) (1.11.0)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  unzip\n",
      "0 upgraded, 1 newly installed, 0 to remove and 23 not upgraded.\n",
      "Need to get 167 kB of archives.\n",
      "After this operation, 558 kB of additional disk space will be used.\n",
      "Get:1 http://mirrors.linode.com/ubuntu bionic/main amd64 unzip amd64 6.0-21ubuntu1 [167 kB]\n",
      "Fetched 167 kB in 0s (10.6 MB/s)\n",
      "Selecting previously unselected package unzip.\n",
      "(Reading database ... 149416 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-21ubuntu1_amd64.deb ...\n",
      "Unpacking unzip (6.0-21ubuntu1) ...\n",
      "Setting up unzip (6.0-21ubuntu1) ...\n",
      "Processing triggers for mime-support (3.60ubuntu1) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install requests\n",
    "!pip install toposort\n",
    "!pip install regex\n",
    "!pip install pandas\n",
    "!pip install tensor2tensor\n",
    "!apt-get install -y unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensor2tensor\n",
      "  Downloading tensor2tensor-1.15.4-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-api-python-client\n",
      "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 19.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-2.1.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 67.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.9.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.18.2)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from tensor2tensor) (2.18.4)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from tensor2tensor) (1.11.0)\n",
      "Collecting dopamine-rl\n",
      "  Downloading dopamine_rl-3.0.1-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 14.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow\n",
      "  Downloading Pillow-7.0.0-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 77.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-gan\n",
      "  Downloading tensorflow_gan-2.0.0-py2.py3-none-any.whl (365 kB)\n",
      "\u001b[K     |████████████████████████████████| 365 kB 78.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1 MB 77.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 76.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gunicorn\n",
      "  Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 22.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (4.43.0)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.2.0.32-cp37-cp37m-manylinux1_x86_64.whl (28.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.2 MB 29.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauth2client\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 25.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.5.1-py2.py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 79.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gevent\n",
      "  Downloading gevent-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 76.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mesh-tensorflow\n",
      "  Downloading mesh_tensorflow-0.1.12-py2.py3-none-any.whl (282 kB)\n",
      "\u001b[K     |████████████████████████████████| 282 kB 79.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-probability==0.7.0\n",
      "  Downloading tensorflow_probability-0.7.0-py2.py3-none-any.whl (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 78.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bz2file\n",
      "  Downloading bz2file-0.98.tar.gz (11 kB)\n",
      "Collecting pypng\n",
      "  Downloading pypng-0.0.20.tar.gz (649 kB)\n",
      "\u001b[K     |████████████████████████████████| 649 kB 75.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gin-config\n",
      "  Downloading gin_config-0.3.0-py3-none-any.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 9.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (2.10.0)\n",
      "Collecting flask\n",
      "  Downloading Flask-1.1.1-py2.py3-none-any.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gym\n",
      "  Downloading gym-0.17.1.tar.gz (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 80.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kfac\n",
      "  Downloading kfac-0.2.0-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 77.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-api-core<2dev,>=1.13.0\n",
      "  Downloading google_api_core-1.16.0-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 28.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-httplib2>=0.0.3\n",
      "  Downloading google_auth_httplib2-0.0.3-py2.py3-none-any.whl (6.3 kB)\n",
      "Collecting google-auth>=1.4.1\n",
      "  Downloading google_auth-1.11.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 17.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/lib/python3/dist-packages (from google-api-python-client->tensor2tensor) (0.9.2)\n",
      "Collecting uritemplate<4dev,>=3.0.0\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.21.1-py2.py3-none-any.whl (31 kB)\n",
      "Collecting attrs>=18.1.0\n",
      "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 80.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (3.11.3)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (1.1.0)\n",
      "Collecting tensorflow-hub>=0.2\n",
      "  Downloading tensorflow_hub-0.7.0-py2.py3-none-any.whl (89 kB)\n",
      "\u001b[K     |████████████████████████████████| 89 kB 28.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn->tensor2tensor) (46.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/lib/python3/dist-packages (from oauth2client->tensor2tensor) (0.2.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/lib/python3/dist-packages (from oauth2client->tensor2tensor) (0.4.2)\n",
      "Collecting rsa>=3.1.4\n",
      "  Downloading rsa-4.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.1.0.tar.gz (512 kB)\n",
      "\u001b[K     |████████████████████████████████| 512 kB 81.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting greenlet>=0.4.14; platform_python_implementation == \"CPython\"\n",
      "  Downloading greenlet-0.4.15-cp37-cp37m-manylinux1_x86_64.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 4.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor) (4.4.2)\n",
      "Collecting cloudpickle>=0.6.1\n",
      "  Downloading cloudpickle-1.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor) (1.0.0)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor) (2.11.1)\n",
      "Requirement already satisfied: click>=5.1 in /usr/lib/python3/dist-packages (from flask->tensor2tensor) (6.7)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 84.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client->tensor2tensor) (2019.3)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis-common-protos-1.51.0.tar.gz (35 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.0.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->tensor2tensor) (1.1.1)\n",
      "Building wheels for collected packages: future, bz2file, pypng, gym, dill, promise, mpmath, googleapis-common-protos\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=7d61c0271bee5b0dabc64f016a9f5225f661fcfeb66200692233556ec6a7e3d1\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for bz2file (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bz2file: filename=bz2file-0.98-py3-none-any.whl size=6882 sha256=7b9e25bab17c977ca96351a9d337e3a5b6ad5da8cc0912ca5a6afcbc1bda2939\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/ce/8d/b5f76b602b16a8a39f2ded74189cf5f09fc4a87bea16c54a8b\n",
      "  Building wheel for pypng (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypng: filename=pypng-0.0.20-py3-none-any.whl size=67162 sha256=9b8a7a52815f2fb269d399ef368a0f06e3371a44db89bdb5aca24ccea356a4ce\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/64/43/dfd10cf95dc1687dc5350e861321ecd9a5d76b7c3d96fa1dc6\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.17.1-py3-none-any.whl size=1648709 sha256=9291976883984b87c33d0eff102592cb71ecc7569299456ae284c4bfa21b5f79\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/7a/2a/2e85bca5dd2c3b319675a5db8a48837b7cfe0603240442b771\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78530 sha256=d3dcee3612e57b2ac920a974b90ad2144f4bd6c64a6310bc8a58b5f4e449e20b\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=0ff851d30a3592072f0ff256ffc0be0d0ac2360702325ddc36c1253401e664a6\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "  Building wheel for mpmath (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mpmath: filename=mpmath-1.1.0-py3-none-any.whl size=532239 sha256=29ba9e23209adea687f7e1274c2882863ac48f81608e7988c915805fe8925a05\n",
      "  Stored in directory: /root/.cache/pip/wheels/e2/46/78/e78f76c356bca9277368f1f97a31b37a8cb937176d9511af31\n",
      "  Building wheel for googleapis-common-protos (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for googleapis-common-protos: filename=googleapis_common_protos-1.51.0-py3-none-any.whl size=77592 sha256=9baf0f41e7224c7de6315315da59ca7604a6c2aa273f2fe2366e1b76b8422a40\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/a1/71/5e427276ceeff277fd76878d1b19fbf4587a2845015d86864b\n",
      "Successfully built future bz2file pypng gym dill promise mpmath googleapis-common-protos\n",
      "\u001b[31mERROR: tensorflow-datasets 2.1.0 has requirement requests>=2.19.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: googleapis-common-protos, cachetools, rsa, google-auth, google-api-core, google-auth-httplib2, uritemplate, google-api-python-client, tensorflow-metadata, attrs, future, dill, promise, tensorflow-datasets, opencv-python, gin-config, scipy, pyglet, cloudpickle, gym, Pillow, dopamine-rl, tensorflow-probability, tensorflow-hub, tensorflow-gan, gunicorn, oauth2client, mpmath, sympy, greenlet, gevent, mesh-tensorflow, bz2file, pypng, itsdangerous, flask, kfac, tensor2tensor\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 17.4.0\n",
      "    Uninstalling attrs-17.4.0:\n",
      "      Successfully uninstalled attrs-17.4.0\n",
      "Successfully installed Pillow-7.0.0 attrs-19.3.0 bz2file-0.98 cachetools-4.0.0 cloudpickle-1.3.0 dill-0.3.1.1 dopamine-rl-3.0.1 flask-1.1.1 future-0.18.2 gevent-1.4.0 gin-config-0.3.0 google-api-core-1.16.0 google-api-python-client-1.8.0 google-auth-1.11.3 google-auth-httplib2-0.0.3 googleapis-common-protos-1.51.0 greenlet-0.4.15 gunicorn-20.0.4 gym-0.17.1 itsdangerous-1.1.0 kfac-0.2.0 mesh-tensorflow-0.1.12 mpmath-1.1.0 oauth2client-4.1.3 opencv-python-4.2.0.32 promise-2.3 pyglet-1.5.0 pypng-0.0.20 rsa-4.0 scipy-1.4.1 sympy-1.5.1 tensor2tensor-1.15.4 tensorflow-datasets-2.1.0 tensorflow-gan-2.0.0 tensorflow-hub-0.7.0 tensorflow-metadata-0.21.1 tensorflow-probability-0.7.0 uritemplate-3.0.1\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Thu Mar 19 23:32:35 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     On   | 00000000:00:02.0 Off |                  Off |\n",
      "| 36%   36C    P8     7W / 260W |      0MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# %tensorflow_version 1.x\n",
    "# !pip install -q gpt-2-simple\n",
    "from tensor2tensor.utils.adafactor import AdafactorOptimizer\n",
    "import gpt_2_simple as gpt2\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 433Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 21.2Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 1.14Git/s]                                                   \n",
      "Fetching model.ckpt.data-00000-of-00001: 6.23Git [01:47, 57.9Mit/s]                                 \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 151Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 2.10Mit [00:00, 38.5Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 29.0Mit/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "# gpt2.download_gpt2(model_name=\"774M\")\n",
    "# gpt2.download_gpt2(model_name=\"355M\")\n",
    "gpt2.download_gpt2(model_name=\"1558M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  python.zip\n",
      "   creating: python/\n",
      "   creating: python/final/\n",
      "   creating: python/final/jsonl/\n",
      "   creating: python/final/jsonl/train/\n",
      "  inflating: python/final/jsonl/train/python_train_9.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_12.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_10.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_0.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_6.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_2.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_4.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_8.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_11.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_5.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_13.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_3.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_1.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_7.jsonl.gz  \n",
      "   creating: python/final/jsonl/test/\n",
      "  inflating: python/final/jsonl/test/python_test_0.jsonl.gz  \n",
      "   creating: python/final/jsonl/valid/\n",
      "  inflating: python/final/jsonl/valid/python_valid_0.jsonl.gz  \n",
      "  inflating: python_dedupe_definitions_v2.pkl  \n",
      "  inflating: python_licenses.pkl     \n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n",
    "\n",
    "!unzip python.zip\n",
    "\n",
    "!gzip -d python/final/jsonl/train/python_train_0.jsonl.gz\n",
    "!gzip -d python/final/jsonl/train/python_train_1.jsonl.gz\n",
    "!gzip -d python/final/jsonl/train/python_train_2.jsonl.gz\n",
    "!gzip -d python/final/jsonl/train/python_train_3.jsonl.gz\n",
    "!gzip -d python/final/jsonl/train/python_train_4.jsonl.gz\n",
    "!gzip -d python/final/jsonl/train/python_train_5.jsonl.gz\n",
    "!gzip -d python/final/jsonl/train/python_train_6.jsonl.gz\n",
    "!gzip -d python/final/jsonl/train/python_train_7.jsonl.gz\n",
    "!gzip -d python/final/jsonl/train/python_train_8.jsonl.gz\n",
    "!gzip -d python/final/jsonl/train/python_train_9.jsonl.gz\n",
    "!gzip -d python/final/jsonl/train/python_train_10.jsonl.gz\n",
    "!gzip -d python/final/jsonl/train/python_train_11.jsonl.gz\n",
    "!gzip -d python/final/jsonl/train/python_train_12.jsonl.gz\n",
    "!gzip -d python/final/jsonl/train/python_train_13.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-730532c3ad6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"python/final/jsonl/train/python_train_{str(i)}.jsonl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mcur2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mall_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1089\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m             )\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "all_sets = []\n",
    "include = 14\n",
    "for i in range(include):\n",
    "  print(i)\n",
    "  fname = f\"python/final/jsonl/train/python_train_{str(i)}.jsonl\"\n",
    "  cur = pd.read_json(fname, lines=True)\n",
    "  cur2 = cur[\"code\"]\n",
    "  all_sets.append(cur2)\n",
    "set1 = pd.concat(all_sets, axis=0)\n",
    "\n",
    "set1 = set1.to_frame()\n",
    "\n",
    "set1[\"length\"] = set1[\"code\"].str.len()\n",
    "set1 = set1[ set1[\"length\"] < 1000]\n",
    "\n",
    "\n",
    "def find_real_funcs(row):\n",
    "  word_list = row[\"code\"].split()\n",
    "  if len(word_list) < 3 or len(word_list[1]) < 2:\n",
    "    return False\n",
    "  else:\n",
    "    if word_list[1][0] == \"_\":\n",
    "      return False\n",
    "    else:\n",
    "      return True\n",
    "\n",
    "set1[\"internal_func\"] = set1.apply(find_real_funcs, axis=1)\n",
    "set1 = set1[ set1[\"internal_func\"]]\n",
    "\n",
    "def find_return_funcs(row):\n",
    "  sent_list = row[\"code\"].split(\"\\n\")\n",
    "  if len(sent_list[-1]) == 0:\n",
    "    return False\n",
    "  else:\n",
    "    if \"return\" in sent_list[-1]:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "\n",
    "set1[\"has_return\"] = set1.apply(find_return_funcs, axis=1)\n",
    "set1 = set1[ set1[\"has_return\"]]\n",
    "set1 = set1.reset_index(drop=True)\n",
    "\n",
    "# def put_return_first(row):\n",
    "#   code = row[\"code\"].split(\"\\n\")\n",
    "#   return_line = code[-1]\n",
    "#   code = code[:-1]\n",
    "#   code.insert(1, return_line)\n",
    "#   code = \"\\n\".join(code)\n",
    "#   return code\n",
    "\n",
    "# set1[\"updated_code\"] = set1.apply( put_return_first, axis=1)\n",
    "# set1 = set1[\"updated_code\"]\n",
    "set1 = set1.sample(frac=1).reset_index(drop=True)\n",
    "set1 = set1[\"code\"]\n",
    "\n",
    "\n",
    "full_frame = set1\n",
    "\n",
    "full_frame = full_frame.astype(str) + \" \\n <|endoftext|> <|startoftext|> \" \n",
    "\n",
    "full_frame.to_csv(\"python/final/jsonl/train/gptready.txt\", index=False, header=False, sep=\"\\n\")\n",
    "full_frame.head()\n",
    "full_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    def face_distance(face_encodings, face_to_comp...\n",
       "1    def load_image_file(file, mode='RGB'):\\n    \"\"...\n",
       "2    def face_locations(img, number_of_times_to_ups...\n",
       "3    def face_encodings(face_image, known_face_loca...\n",
       "4    def to_arrow_schema(schema):\\n    \"\"\" Convert ...\n",
       "Name: code, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:56<00:00, 176.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing text_encoded.npz\n"
     ]
    }
   ],
   "source": [
    "# file_name = \"python/final/jsonl/train/gptready.txt\"\n",
    "# gpt2.encode_dataset(file_name, model_name=\"1558M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint    python\t\t\t\tpython.zip\t  train.ipynb\r\n",
      "gpt_2_simple  python_dedupe_definitions_v2.pkl\ttext_encoded.npz\r\n",
      "models\t      python_licenses.pkl\t\ttraining.py\r\n"
     ]
    }
   ],
   "source": [
    "!cd models\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://files.srl.inf.ethz.ch/data/py150_files.tar.gz\n",
    "!tar -xvzf py150_files.tar.gz\n",
    "!tar -xvzf data.tar.gz\n",
    "file_name = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.encode_dataset(file_name, model_name=\"1558M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/codegen/codegen/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /root/codegen/codegen/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
      "Instructions for updating:\n",
      "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
      "Loading checkpoint models/1558M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/1558M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:58<00:00, 178.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 36463609 tokens\n",
      "Training...\n",
      "[10 | 53.08] loss=1.54 avg=1.54\n",
      "[20 | 74.06] loss=1.09 avg=1.31\n",
      "[30 | 95.33] loss=1.54 avg=1.39\n",
      "[40 | 116.76] loss=1.61 avg=1.45\n",
      "[50 | 138.28] loss=1.31 avg=1.42\n"
     ]
    }
   ],
   "source": [
    "file_name = \"text_encoded.npz\"\n",
    "# file_name = \"python/final/jsonl/train/gptready.txt\"\n",
    "full_frame = None\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "# 774M\n",
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name='1558M',\n",
    "              steps=-1,\n",
    "              restore_from='fresh',\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              sample_every=250,\n",
    "              save_every=500,\n",
    "              use_memory_saving_gradients=True,\n",
    "              only_train_transformer_layers=True,\n",
    "              accumulate_gradients=1,\n",
    "              optimizer=\"adafactor\",\n",
    "              fp16=False,\n",
    "              output_checkpoint=True,\n",
    "              batch_size=1\n",
    "              \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"text_encoded.npz\"\n",
    "# file_name = \"python/final/jsonl/train/gptready.txt\"\n",
    "full_frame = None\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "# 774M\n",
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name='1558M',\n",
    "              steps=-1,\n",
    "              restore_from='latest',\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              sample_every=250,\n",
    "              save_every=500,\n",
    "              use_memory_saving_gradients=True,\n",
    "              only_train_transformer_layers=True,\n",
    "              accumulate_gradients=1,\n",
    "              optimizer=\"adafactor\",\n",
    "              fp16=False,\n",
    "              output_checkpoint=False,\n",
    "              batch_size=1,\n",
    "              overwrite=True\n",
    "              \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.load_gpt2(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.generate(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.generate(sess,\n",
    "              run_name = \"run1\",\n",
    "              prefix=\"def join_text(text_list):\",\n",
    "              nsamples=20,\n",
    "              batch_size=5,\n",
    "              top_p = 0.9,\n",
    "              temperature = 0.8\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}